{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a717b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc7a299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Model Configuration ---\n",
    "\n",
    "# Hyperparameters for our toy model. These are kept small to ensure\n",
    "# it runs quickly on a standard laptop CPU.\n",
    "VOCAB_SIZE = 30522  # Standard for bert-base-uncased tokenizer\n",
    "EMBED_DIM = 128     # Dimension of token embeddings\n",
    "HIDDEN_DIM = 256    # Dimension of the expert's hidden layer\n",
    "NUM_EXPERTS = 4     # The number of experts in our MoE layer\n",
    "TOP_K = 2           # Number of experts to route each token to\n",
    "NUM_CLASSES = 4    # For the 20 Newsgroups dataset\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0357dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. MoE Model Definition ---\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"A simple feed-forward network to be used as an expert.\"\"\"\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim) # Project back to the original dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The core Mixture-of-Experts layer.\n",
    "    This layer takes a batch of tokens and routes each token to the top-k experts.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, hidden_dim, num_experts, top_k):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Create the pool of experts\n",
    "        self.experts = nn.ModuleList([Expert(embed_dim, hidden_dim) for _ in range(num_experts)])\n",
    "        \n",
    "        # The gating network (router) is a simple linear layer that outputs\n",
    "        # a logit for each expert.\n",
    "        self.router = nn.Linear(embed_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, embed_dim)\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "\n",
    "        # Reshape the input to be a flat list of tokens for easier processing\n",
    "        x_flat = x.view(-1, embed_dim) # Shape: (batch_size * seq_len, embed_dim)\n",
    "        num_tokens = x_flat.shape[0]\n",
    "\n",
    "        # 1. Gating / Routing\n",
    "        # Get the logits from the router for each token\n",
    "        router_logits = self.router(x_flat) # Shape: (num_tokens, num_experts)\n",
    "\n",
    "        # Find the top-k experts and their corresponding routing weights\n",
    "        routing_weights, chosen_expert_indices = torch.topk(router_logits, self.top_k, dim=-1)\n",
    "        routing_weights = F.softmax(routing_weights, dim=-1) # Softmax over the top-k logits\n",
    "\n",
    "        # 2. Expert Processing\n",
    "        # Initialize the final output tensor\n",
    "        final_output = torch.zeros_like(x_flat)\n",
    "        \n",
    "        # Create a flat index to map tokens to their chosen experts\n",
    "        flat_expert_indices = chosen_expert_indices.view(-1)\n",
    "        \n",
    "        # Create a tensor that maps each token to its position in the batch\n",
    "        token_batch_map = torch.arange(num_tokens, device=x.device).repeat_interleave(self.top_k)\n",
    "\n",
    "        # Get the expert outputs for all tokens and all chosen experts\n",
    "        # This is a more complex but efficient way to handle batching for top-k\n",
    "        expert_outputs = torch.zeros_like(x_flat)\n",
    "        for i in range(self.num_experts):\n",
    "            # Find which tokens have this expert in their top-k list\n",
    "            mask = (chosen_expert_indices == i).any(dim=-1)\n",
    "            if mask.any():\n",
    "                expert_outputs[mask] = self.experts[i](x_flat[mask])\n",
    "\n",
    "        # Combine the expert outputs using the routing weights\n",
    "        # We need to gather the correct expert outputs for each token\n",
    "        # and multiply by the corresponding routing weight.\n",
    "        weighted_outputs = torch.zeros_like(x_flat)\n",
    "        for i in range(self.top_k):\n",
    "            expert_idx = chosen_expert_indices[:, i]\n",
    "            weight = routing_weights[:, i].unsqueeze(1)\n",
    "            \n",
    "            # Gather the outputs from the correct experts\n",
    "            # This is a bit complex, but it avoids a slow loop\n",
    "            current_expert_outputs = torch.zeros_like(x_flat)\n",
    "            for j in range(self.num_experts):\n",
    "                mask = (expert_idx == j)\n",
    "                if mask.any():\n",
    "                    current_expert_outputs[mask] = self.experts[j](x_flat[mask])\n",
    "            \n",
    "            weighted_outputs += weight * current_expert_outputs\n",
    "\n",
    "        # Reshape the output back to the original input shape\n",
    "        return weighted_outputs.view(batch_size, seq_len, embed_dim)\n",
    "\n",
    "\n",
    "class TinyMoEForClassification(nn.Module):\n",
    "    \"\"\"The main model that uses the MoE layer for classification.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_experts, top_k, num_classes):\n",
    "        super().__init__()\n",
    "        # The embedding layer turns token IDs into dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Our custom MoE layer\n",
    "        self.moe_layer = MoELayer(embed_dim, hidden_dim, num_experts, top_k)\n",
    "        \n",
    "        # A simple linear layer to map the output of the MoE layer to class predictions\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # input_ids shape: (batch_size, sequence_length)\n",
    "        embedded = self.embedding(input_ids) # Shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        moe_output = self.moe_layer(embedded) # Shape: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # We use the representation of the first token ([CLS] token) for classification.\n",
    "        # This is a common practice in models like BERT.\n",
    "        cls_token_output = moe_output[:, 0] # Shape: (batch_size, embed_dim)\n",
    "\n",
    "        # Get the final logits for each class\n",
    "        logits = self.classifier(cls_token_output) # Shape: (batch_size, num_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d22438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Data Preparation ---\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "# Load a small, fast tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "# OLD\n",
    "# dataset = load_dataset(\"SetFit/20_newsgroups\")\n",
    "\n",
    "# NEW\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "# OLD\n",
    "# class_names = ['alt.atheism', 'comp.graphics', ...] # (20 names)\n",
    "\n",
    "# NEW\n",
    "class_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "# Define a collate function to process batches of data for the DataLoader\n",
    "def collate_fn(batch):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "    \n",
    "    # The tokenizer handles padding, truncation, and tensor conversion\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\", # Pad to a fixed length\n",
    "        truncation=True,\n",
    "        max_length=256 # Use a smaller max_length for faster training\n",
    "    )\n",
    "    \n",
    "    inputs['labels'] = torch.tensor(labels)\n",
    "    return inputs\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Data preparation complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9faa3a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 3750/3750 [10:29<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Average Training Loss: 1.3868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 3750/3750 [13:54<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Average Training Loss: 1.3865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 3750/3750 [12:32<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Average Training Loss: 1.3865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 3750/3750 [12:23<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Average Training Loss: 1.3865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 3750/3750 [12:14<00:00,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Average Training Loss: 1.3864\n",
      "\n",
      "--- Training Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = TinyMoEForClassification(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_experts=NUM_EXPERTS,\n",
    "    top_k=TOP_K,\n",
    "    num_classes=NUM_CLASSES\n",
    ").to(DEVICE)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Use tqdm for a nice progress bar during training\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        # Move batch to the device\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} | Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c43df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Basic Analysis Example ---\n",
      "Expert utilization for one sample batch:\n",
      "  Expert 0: 422 tokens\n",
      "  Expert 1: 6700 tokens\n",
      "  Expert 2: 535 tokens\n",
      "  Expert 3: 535 tokens\n"
     ]
    }
   ],
   "source": [
    "# --- Basic Analysis Example ---\n",
    "# Now you can use this trained model as the subject for your MoE-Diag toolkit.\n",
    "# Here's a quick example of how you might inspect the router for a single batch.\n",
    "print(\"\\n--- Running Basic Analysis Example ---\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get one batch from the test set\n",
    "    sample_batch = next(iter(test_loader))\n",
    "    input_ids = sample_batch['input_ids'].to(DEVICE)\n",
    "    \n",
    "    # Manually perform the first few steps of the forward pass to get router logits\n",
    "    embedded = model.embedding(input_ids)\n",
    "    x_flat = embedded.view(-1, EMBED_DIM)\n",
    "    router_logits = model.moe_layer.router(x_flat)\n",
    "    routing_weights = F.softmax(router_logits, dim=1)\n",
    "    chosen_expert_indices = torch.argmax(routing_weights, dim=1)\n",
    "\n",
    "    # Count how many tokens were assigned to each expert in this batch\n",
    "    expert_counts = torch.bincount(chosen_expert_indices, minlength=NUM_EXPERTS)\n",
    "    \n",
    "    print(\"Expert utilization for one sample batch:\")\n",
    "    for i, count in enumerate(expert_counts):\n",
    "        print(f\"  Expert {i}: {count.item()} tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "873decc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Ablation Analysis Function ---\n",
    "\n",
    "def run_ablation_analysis(model, test_loader, device, num_classes, num_experts, class_names):\n",
    "    \"\"\"\n",
    "    Performs counterfactual analysis by ablating each expert one by one and\n",
    "    measuring the impact on per-class accuracy.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Ablation Analysis ---\")\n",
    "    model.eval()\n",
    "    \n",
    "    def evaluate_per_class_accuracy():\n",
    "        \"\"\"Helper function to evaluate the model and return per-class accuracy.\"\"\"\n",
    "        class_correct = [0] * num_classes\n",
    "        class_totals = [0] * num_classes\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Evaluating\", leave=False):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                for i in range(len(labels)):\n",
    "                    label = labels[i]\n",
    "                    class_correct[label] += (predicted[i] == label).item()\n",
    "                    class_totals[label] += 1\n",
    "        \n",
    "        # Calculate accuracy for each class, avoiding division by zero\n",
    "        accuracies = [(class_correct[i] / class_totals[i]) * 100 if class_totals[i] > 0 else 0 \n",
    "                      for i in range(num_classes)]\n",
    "        return accuracies\n",
    "\n",
    "    # 1. Get baseline accuracy with the full model\n",
    "    print(\"Calculating baseline accuracy...\")\n",
    "    baseline_accuracies = evaluate_per_class_accuracy()\n",
    "    \n",
    "    # --- NEW: Print baseline accuracies for context ---\n",
    "    print(\"\\nBaseline Per-Class Accuracy (%):\")\n",
    "    baseline_df = pd.DataFrame([baseline_accuracies], \n",
    "                               columns=[name[:15] for name in class_names], \n",
    "                               index=['Baseline Acc.'])\n",
    "    print(baseline_df.round(2))\n",
    "\n",
    "\n",
    "    # 2. Iterate through each expert, ablate it, and re-evaluate\n",
    "    ablation_results = []\n",
    "    for expert_to_ablate in range(num_experts):\n",
    "        print(f\"\\nAblating Expert {expert_to_ablate}...\")\n",
    "        \n",
    "        # Store original weights to restore them later\n",
    "        original_weights = [p.clone().detach() for p in model.moe_layer.experts[expert_to_ablate].parameters()]\n",
    "\n",
    "        # Zero out the weights of the current expert\n",
    "        with torch.no_grad():\n",
    "            for param in model.moe_layer.experts[expert_to_ablate].parameters():\n",
    "                param.data.fill_(0)\n",
    "        \n",
    "        # Evaluate the model with the ablated expert\n",
    "        ablated_accuracies = evaluate_per_class_accuracy()\n",
    "        ablation_results.append(ablated_accuracies)\n",
    "\n",
    "        # IMPORTANT: Restore the original weights\n",
    "        with torch.no_grad():\n",
    "            for i, param in enumerate(model.moe_layer.experts[expert_to_ablate].parameters()):\n",
    "                param.data.copy_(original_weights[i])\n",
    "        print(f\"Restored Expert {expert_to_ablate}.\")\n",
    "\n",
    "    # 3. Present the results in a clear table\n",
    "    print(\"\\n--- Ablation Analysis Results ---\")\n",
    "    \n",
    "    # Calculate the accuracy drop (Baseline - Ablated)\n",
    "    accuracy_drops = []\n",
    "    for expert_accuracies in ablation_results:\n",
    "        drop = [base - ablated for base, ablated in zip(baseline_accuracies, expert_accuracies)]\n",
    "        accuracy_drops.append(drop)\n",
    "\n",
    "    # Use pandas to create a DataFrame for nice formatting\n",
    "    df = pd.DataFrame(accuracy_drops,\n",
    "                      columns=[name[:15] for name in class_names], # Truncate long class names\n",
    "                      index=[f\"Ablate Expert {i}\" for i in range(num_experts)])\n",
    "\n",
    "    # Display the accuracy drop. A high positive number means ablating that expert\n",
    "    # significantly hurt performance for that class, indicating specialization.\n",
    "    print(\"\\nAccuracy Drop (%) After Ablating Each Expert:\")\n",
    "    pd.set_option('display.width', 1000) # Widen pandas output\n",
    "    print(df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59e1b1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Ablation Analysis ---\n",
      "Calculating baseline accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Per-Class Accuracy (%):\n",
      "               World  Sports  Business  Sci/Tech\n",
      "Baseline Acc.    0.0     0.0       0.0     100.0\n",
      "\n",
      "Ablating Expert 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored Expert 0.\n",
      "\n",
      "Ablating Expert 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored Expert 1.\n",
      "\n",
      "Ablating Expert 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored Expert 2.\n",
      "\n",
      "Ablating Expert 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored Expert 3.\n",
      "\n",
      "--- Ablation Analysis Results ---\n",
      "\n",
      "Accuracy Drop (%) After Ablating Each Expert:\n",
      "                 World  Sports  Business  Sci/Tech\n",
      "Ablate Expert 0    0.0     0.0       0.0       0.0\n",
      "Ablate Expert 1    0.0     0.0       0.0       0.0\n",
      "Ablate Expert 2    0.0     0.0       0.0       0.0\n",
      "Ablate Expert 3    0.0     0.0       0.0       0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "run_ablation_analysis(model, test_loader, DEVICE, NUM_CLASSES, NUM_EXPERTS, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37dac02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
