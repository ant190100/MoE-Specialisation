# MoE Specialisation Research

A research repository for analyzing expert specialization in Mixture of Experts (MoE) models, designed for both local development and HPC cluster execution.

**Repository Status**: âœ… **Clean & Optimized** - All redundancies removed, structure validated, tests passing.

## Repository Structure

```
MoE-Specialisation/
â”œâ”€â”€ src/                          # Core source code
â”‚   â”œâ”€â”€ models/                   # Model implementations
â”‚   â”‚   â”œâ”€â”€ expert.py            # Individual expert networks
â”‚   â”‚   â”œâ”€â”€ moe_layer.py         # MoE layer implementation
â”‚   â”‚   â””â”€â”€ tiny_moe.py          # Complete TinyMoE model
â”‚   â”œâ”€â”€ data/                    # Data handling utilities
â”‚   â”‚   â””â”€â”€ loaders.py           # Dataset loaders and collate functions
â”‚   â”œâ”€â”€ analysis/                # Analysis tools
â”‚   â”‚   â”œâ”€â”€ ablation.py          # Ablation analysis
â”‚   â”‚   â””â”€â”€ visualization.py     # Plotting and visualization
â”‚   â””â”€â”€ utils/                   # Utility functions
â”‚       â””â”€â”€ config.py            # Configuration management
â”œâ”€â”€ experiments/                 # Local experiment scripts
â”‚   â””â”€â”€ toy_model_experiment.py  # Toy model training and analysis
â”œâ”€â”€ hpc/                         # HPC-specific code
â”‚   â”œâ”€â”€ experiments/             # Experiment scripts for cluster
â”‚   â”‚   â”œâ”€â”€ base_experiment.py   # Base experiment class
â”‚   â”‚   â””â”€â”€ tinymistral_experiment.py
â”‚   â”œâ”€â”€ job_scripts/             # SLURM job scripts
â”‚   â”‚   â””â”€â”€ tinymistral_analysis.slurm
â”‚   â””â”€â”€ configs/                 # HPC-specific configurations
â”‚       â””â”€â”€ tinymistral_config.yaml
â”œâ”€â”€ analysis_scripts/            # Local analysis tools
â”‚   â”œâ”€â”€ local_analysis.py        # Process HPC results locally
â”‚   â””â”€â”€ sync_hpc.py             # Sync files with HPC
â”œâ”€â”€ tests/                       # Testing infrastructure
â”‚   â”œâ”€â”€ test_local.py           # Comprehensive local testing
â”‚   â””â”€â”€ test_structure.py       # Repository structure validation
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for exploration
â”‚   â””â”€â”€ toy_model_workflow.ipynb # Original workflow notebook
â”œâ”€â”€ results/                     # Results storage (git-ignored)
â”‚   â””â”€â”€ {experiment_name}/       # Individual experiment results
â”‚       â”œâ”€â”€ models/              # Trained model weights
â”‚       â”œâ”€â”€ analysis/            # Analysis outputs and visualizations
â”‚       â”œâ”€â”€ logs/                # Execution logs
â”‚       â””â”€â”€ configs/             # Experiment configurations
â”œâ”€â”€ configs/                     # Configuration files
â”‚   â””â”€â”€ toy_model_config.yaml   # Local toy model config
â””â”€â”€ requirements/                # Dependencies
    â”œâ”€â”€ local.txt               # Local development deps
    â””â”€â”€ hpc.txt                 # HPC cluster deps
```

## Workflow

### 1. Local Development & Testing

```bash
# Install dependencies
pip install -r requirements/local.txt

# Quick local test with comprehensive validation
python tests/test_local.py

# Validate repository structure
python tests/test_structure.py

# Work with notebooks
jupyter lab notebooks/
```

### 2. HPC Execution

```bash
# Submit TinyMistral analysis
sbatch hpc/job_scripts/tinymistral_analysis.slurm

# Check job status
squeue -u $USER
```

### 3. Local Analysis of HPC Results

```bash
# Sync results from HPC
python analysis_scripts/sync_hpc.py --direction from-hpc --host your-cluster.edu --remote-path /scratch/user/results

# Analyze toy model results (local experiments)
python analysis_scripts/local_analysis.py --experiment results/toy_model_analysis --type toy_model

# Analyze TinyMistral results (HPC experiments)  
python analysis_scripts/local_analysis.py --experiment results/tinymistral_analysis --type tinymistral

# Compare multiple experiments
python analysis_scripts/local_analysis.py --compare results/toy_model_analysis results/tinymistral_analysis
```

## Configuration

### Toy Model Config (`configs/toy_model_config.yaml`)
```yaml
EMBED_DIM: 32
HIDDEN_DIM: 64
NUM_EXPERTS: 4
TOP_K: 2
NUM_CLASSES: 4
DATASET: "ag_news"
```

### HPC Config (`hpc/configs/tinymistral_config.yaml`)
```yaml
MODEL_PATH: "/scratch/user/models/TinyMistral-6x248M"
NUM_EXPERTS: 6
NUM_CLASSES: 20
DATASET: "20_newsgroups"
HPC_RESULTS_DIR: "/scratch/user/results"
```

## Key Features

### ğŸ”¬ **Ablation Analysis**
- Systematic expert ablation to measure specialization
- Per-class accuracy impact measurement
- Statistical analysis and visualization

### ğŸ“Š **Visualizations**
- Expert specialization heatmaps
- Routing entropy distributions
- Expert utilization statistics
- Confidence analysis plots

### ğŸ–¥ï¸ **HPC Integration**
- SLURM job scripts for cluster execution
- Experiment tracking and logging
- Result synchronization utilities
- Configurable resource requirements

### ğŸ”§ **Modular Design**
- Reusable model components
- Configurable experiments
- Extensible analysis framework
- Clean separation of concerns

## Getting Started

1. **Clone and setup:**
   ```bash
   git clone <repo-url>
   cd MoE-Specialisation
   pip install -e .
   ```

2. **Local testing:**
   ```bash
   # Run comprehensive tests
   python tests/test_local.py
   
   # Validate structure
   python tests/test_structure.py
   
   # Run local toy model experiment
   python experiments/toy_model_experiment.py
   ```

3. **HPC usage:**
   - Modify `hpc/configs/` for your cluster
   - Update SLURM scripts with your paths
   - Submit jobs and sync results

4. **Analysis:**
   ```bash
   python analysis_scripts/local_analysis.py --help
   ```

## Research Applications

This framework supports research into:
- **Expert Specialization**: How do experts specialize across different domains?
- **Routing Patterns**: What drives expert selection in MoE models?
- **Scalability**: How does specialization change with model size?
- **Transfer Learning**: Do specialization patterns transfer across tasks?

## Models Supported

- **TinyMoE**: Custom lightweight MoE for quick experimentation
- **TinyMistral**: Pre-trained MoE model analysis
- **Extensible**: Easy to add new MoE architectures

## Contributing

1. Add new models in `src/models/`
2. Extend analysis in `src/analysis/`
3. Create experiment scripts in `hpc/experiments/`
4. Add visualization in `src/analysis/visualization.py`

## License

MIT License